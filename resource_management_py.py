# -*- coding: utf-8 -*-
"""Resource management.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14iAstk0nQQWmkdzDL1FPqr5mm_YSAQ_7

## EDA

##package install
"""

# Install if needed:
!pip install pandas numpy plotly streamlit scikit-learn nltk spacy openpyxl

import os, glob, zipfile
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD

"""## DRIVE MOUNTED"""

from google.colab import drive
    drive.mount('/content/drive')

"""##SCIKIT PACKAGE"""

# (run in terminal or notebook cell with !)
!pip install pandas numpy matplotlib seaborn plotly scikit-learn nltk ydata-profiling python-pptx

# utils.py
import os
import glob
import pandas as pd
from typing import List

def list_csv_files(data_dir: str) -> List[str]:
    """Return list of csv/xls/xlsx files in data_dir."""
    patterns = ["*.csv", "*.xlsx", "*.xls"]
    files = []
    for p in patterns:
        files.extend(glob.glob(os.path.join(data_dir, p)))
    return sorted(files)

def read_csv_or_excel(path: str) -> pd.DataFrame:
    """Read csv or excel into DataFrame, try common encodings."""
    if path.lower().endswith(".csv"):
        return pd.read_csv(path, low_memory=False)
    else:
        return pd.read_excel(path)

"""## unzip file"""

import zipfile, os

zip_path = "/DataSets-20250910T135139Z-1-001.zip"   # change to your uploaded file name
extract_to = "/content"

# make sure target folder exists
os.makedirs(extract_to, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

# List first few extracted files
for root, dirs, files in os.walk(extract_to):
    for f in files[:20]:   # preview first 20 files
        print(os.path.join(root, f))

"""##EDA"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np # Import numpy

# Load dataset
df = pd.read_csv("/content/DataSets/DDW_B18_0800_NIC_FINAL_STATE_RAJASTHAN-2011.csv", encoding='latin1')

# 1. Basic info
print(df.shape)         # rows, columns
print(df.info())        # data types, nulls
print(df.head())        # first 5 rows

# 2. Summary stats
print(df.describe())

# 3. Missing values
print(df.isnull().sum())

# 4. Univariate analysis
# Ensure 'age' column exists before plotting
if 'age' in df.columns:
    df['age'].hist(bins=20)
    plt.title("Age Distribution")
    plt.show()
else:
    print("Warning: 'age' column not found for distribution plot.")


# 5. Categorical variable analysis
# Ensure 'gender' column exists before analysis
if 'gender' in df.columns:
    print(df['gender'].value_counts())
    sns.countplot(x='gender', data=df)
    plt.show()
else:
     print("Warning: 'gender' column not found for categorical analysis.")


# 6. Correlation
# Select only numeric columns for correlation matrix
numeric_df = df.select_dtypes(include=np.number)
if not numeric_df.empty:
    plt.figure(figsize=(8,6))
    sns.heatmap(numeric_df.corr(), annot=True, cmap="coolwarm")
    plt.show()
else:
    print("Warning: No numeric columns found for correlation analysis.")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("/content/DataSets/DDW_B18_1200_NIC_FINAL_STATE_ARUNACHAL_PRADESH-2011.csv", encoding='latin1')

# 1. Basic info
print(df.shape)         # rows, columns
print(df.info())        # data types, nulls
print(df.head())        # first 5 rows

# 2. Summary stats
print(df.describe())

# 3. Missing values
print(df.isnull().sum())

# 4. Univariate analysis
# Ensure 'age' column exists before plotting
if 'age' in df.columns:
    df['age'].hist(bins=20)
    plt.title("Age Distribution")
    plt.show()
else:
    print("Warning: 'age' column not found for distribution plot.")


# 5. Categorical variable analysis
# Ensure 'gender' column exists before analysis
if 'gender' in df.columns:
    print(df['gender'].value_counts())
    sns.countplot(x='gender', data=df)
    plt.show()
else:
     print("Warning: 'gender' column not found for categorical analysis.")


# 6. Correlation
# Select only numeric columns for correlation matrix
numeric_df = df.select_dtypes(include=np.number)
if not numeric_df.empty:
    plt.figure(figsize=(8,6))
    sns.heatmap(numeric_df.corr(), annot=True, cmap="coolwarm")
    plt.show()
else:
    print("Warning: No numeric columns found for correlation analysis.")

import os, glob
import pandas as pd

# Path to unzipped dataset
DATA_DIR = "/content/DataSets"   # Change to the directory where the zip was extracted

# 1. List all CSV/XLSX files in the folder
def list_data_files(data_dir=DATA_DIR):
    patterns = ["*.csv", "*.xlsx", "*.xls"]
    files = []
    for p in patterns:
        files.extend(glob.glob(os.path.join(data_dir, p)))
    return sorted(files)

files = list_data_files()
print("Found files:", files)

# 2. Load safely into pandas
def read_file_safe(path):
    if path.lower().endswith(".csv"):
        return pd.read_csv(path, low_memory=False)
    else:
        return pd.read_excel(path)

dfs = []
for f in files:
    try:
        df_temp = read_file_safe(f)
        df_temp['__source_file'] = os.path.basename(f)  # track source
        dfs.append(df_temp)
        print(f"Loaded {f} ‚Üí shape {df_temp.shape}")
    except Exception as e:
        print(f"Could not read {f}: {e}")

# 3. Merge into single DataFrame
if dfs:
    df = pd.concat(dfs, ignore_index=True, sort=False)
    print("Merged dataset shape:", df.shape)
else:
    raise SystemExit("No datasets could be loaded!")

import pandas as pd

def read_file_safe(path):
    try:
        if path.lower().endswith(".csv"):
            return pd.read_csv(path, encoding="utf-8", low_memory=False)
        else:
            return pd.read_excel(path)
    except UnicodeDecodeError:
        # fallback for non-UTF-8 files
        if path.lower().endswith(".csv"):
            return pd.read_csv(path, encoding="latin1", low_memory=False)
        else:
            return pd.read_excel(path)

import os, glob

DATA_DIR = "/content/DataSets"   # adjust path

def list_data_files(data_dir=DATA_DIR):
    patterns = ["*.csv", "*.xlsx", "*.xls"]
    files = []
    for p in patterns:
        files.extend(glob.glob(os.path.join(data_dir, p)))
    return sorted(files)

files = list_data_files()
print("Found files:", len(files))

dfs = []
for f in files:
    try:
        df_temp = read_file_safe(f)
        df_temp['__source_file'] = os.path.basename(f)
        dfs.append(df_temp)
        print(f"Loaded {f} ‚Üí shape {df_temp.shape}")
    except Exception as e:
        print(f"‚ùå Could not read {f}: {e}")

# Merge
if dfs:
    df = pd.concat(dfs, ignore_index=True, sort=False)
    print("‚úÖ Final merged dataset shape:", df.shape)
else:
    print("No datasets could be loaded")

import os, glob
import pandas as pd

DATA_DIR = "/content/DataSets"   # adjust path

def list_data_files(data_dir=DATA_DIR):
    patterns = ["*.csv", "*.xlsx", "*.xls"]
    files = []
    for p in patterns:
        files.extend(glob.glob(os.path.join(data_dir, p)))
    return sorted(files)

def read_file_safe(path):
    """Try reading CSV or Excel safely with encoding fallback."""
    try:
        if path.lower().endswith(".csv"):
            return pd.read_csv(path, encoding="utf-8", low_memory=False)
        else:
            return pd.read_excel(path)
    except UnicodeDecodeError:
        return pd.read_csv(path, encoding="latin1", low_memory=False)

files = list_data_files()
print("Found files:", len(files))

dfs = []
for f in files:
    try:
        df_temp = read_file_safe(f)
        df_temp['__source_file'] = os.path.basename(f)
        dfs.append(df_temp)
        print(f"Loaded {f} ‚Üí shape {df_temp.shape}")
    except Exception as e:
        print(f"‚ùå Could not read {f}: {e}")

# Merge & save
if dfs:
    df = pd.concat(dfs, ignore_index=True, sort=False)
    print("‚úÖ Final merged dataset shape:", df.shape)

    # Save as CSV
    out_path = "/content/merged_dataset.csv"
    df.to_csv(out_path, index=False)
    print("üíæ Saved merged dataset to:", out_path)

    # (Optional) Save as Excel
    df.to_excel("/content/merged_dataset.xlsx", index=False)
    print("üíæ Also saved as Excel:", "/content/merged_dataset.xlsx")
else:
    print("No datasets could be loaded")

merged_df.to_csv("/content/merged_dataset.csv", index=False)
print("‚úÖ Merged dataset saved as /content/merged_dataset.csv")

# Save merged dataset to /content
output_path = "/content/merged_dataset.csv"
df.to_csv(output_path, index=False)

print(f"‚úÖ Merged dataset saved at: {output_path}")

print("Rows,cols:", df.shape)
print("\nColumns:\n", df.columns.tolist())
print("\nDtypes:\n", df.dtypes)
print("\nFirst rows:\n", df.head().T)

# Missing values
print("\nMissing values per column:")
print(df.isnull().sum().sort_values(ascending=False))

# Shape
print("Rows, cols:", df.shape)

# Column names
print("\nColumns:\n", df.columns.tolist())

# Data types
print("\nDtypes:\n", df.dtypes)

# First few rows (transposed for readability)
print("\nFirst rows:\n", df.head().T)

# Missing values summary
print("\nMissing values per column:")
print(df.isnull().sum().sort_values(ascending=False))

df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]

expected = ["state", "industry", "sex", "worker_type", "count"]
print("Detected expected columns present:", {col: (col in df.columns) for col in expected})

# If count is not present but a numeric column exists, map it:
if "count" not in df.columns:
    num_cols = df.select_dtypes(include='number').columns.tolist()
    if num_cols:
        print("No 'count' column; using numeric column:", num_cols[0])
        df = df.rename(columns={num_cols[0]: "count"})
    else:
        raise SystemExit("No numeric column found to use as 'count'.")

# Normalize text columns if present
for col in ["state", "industry", "sex", "worker_type"]:
    if col in df.columns:
        df[col] = df[col].astype(str).str.strip()

# Convert count to integer
df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0).astype(int)

# Drop exact duplicates (optional)
dup_before = df.shape[0]
df = df.drop_duplicates()
print("Dropped duplicates:", dup_before - df.shape[0])

# Numeric summary
print(df['count'].describe())

# Histogram
plt.figure(figsize=(6,4))
sns.histplot(df['count'], bins=30)
plt.title("Distribution of 'count'")
plt.show()

# Top categories
for cat in ["state", "industry", "sex", "worker_type"]:
    if cat in df.columns:
        print(f"\nTop values for {cat}:")
        print(df[cat].value_counts().head(10))
        sns.countplot(y=cat, data=df, order=df[cat].value_counts().index[:10])
        plt.title(f"Top 10 {cat}")
        plt.show()

# Aggregation: total workers per state
if 'state' in df.columns:
    st = df.groupby('state', as_index=False)['count'].sum().sort_values('count', ascending=False)
    print(st.head(15))
    fig = px.bar(st.head(15), x='count', y='state', orientation='h', title='Top 15 states by worker count')
    fig.show()

# Male vs female by state (if sex present)
if 'sex' in df.columns and 'state' in df.columns:
    mf = df.groupby(['state','sex'], as_index=False)['count'].sum()
    fig = px.bar(mf, x='state', y='count', color='sex', title='Male vs Female by State')
    fig.update_layout(xaxis={'categoryorder':'total descending'})
    fig.show()

# Top industries
if 'industry' in df.columns:
    ind = df.groupby('industry', as_index=False)['count'].sum().sort_values('count', ascending=False)
    print(ind.head(20))
    fig = px.bar(ind.head(20), x='count', y='industry', orientation='h', title='Top 20 industries')
    fig.show()

# Standardize column names first
df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]

# Check and standardize potential 'state' and 'industry' columns
# Standardize 'state' column
potential_state_cols = ['state/ut', 'india/states', 'state']
state_col = None
for col in potential_state_cols:
    if col in df.columns:
        state_col = col
        break
if state_col and state_col != 'state':
     df.rename(columns={state_col: "state"}, inplace=True)
elif not state_col and 'state' not in df.columns:
    print("Warning: No 'state' column found or inferable.")


# Standardize 'industry' column
potential_industry_cols = ['industry_name', 'industry_description', 'description', 'division_group', 'group', 'nic_name'] # Added nic_name based on dataframe head
industry_col = None
for col in potential_industry_cols:
    if col in df.columns:
        industry_col = col
        break

if industry_col and industry_col != 'industry':
     df.rename(columns={industry_col: "industry"}, inplace=True)
elif not industry_col and 'industry' not in df.columns:
     print("Warning: No 'industry' column found or inferable.")

# Also standardize 'count' column to ensure it exists
potential_count_cols = ['count', 'persons', 'workers', 'total']
count_col = None
for col in potential_count_cols:
    if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
         count_col = col
         break
# If no numeric count column found, check for non-numeric columns that might contain counts
if count_col is None:
     for col in potential_count_cols:
         if col in df.columns:
              count_col = col
              break

if count_col and count_col != 'count':
     df.rename(columns={count_col: "count"}, inplace=True)
elif not count_col and 'count' not in df.columns:
     print("Warning: No 'count' column found or inferable.")


# Rows with missing key info - Now check for the standardized column names
key_cols_to_check = []
if 'state' in df.columns:
    key_cols_to_check.append('state')
if 'industry' in df.columns:
    key_cols_to_check.append('industry')
if 'count' in df.columns:
    key_cols_to_check.append('count')


if key_cols_to_check:
    key_missing = df[df[key_cols_to_check].isnull().any(axis=1)]
    print("Rows missing key info:", key_missing.shape[0])
    # Example imputation: drop rows with empty industry OR state
    # Only drop if 'state' or 'industry' columns exist
    subset_cols_to_drop = [col for col in ['state', 'industry'] if col in df.columns]
    if subset_cols_to_drop:
         df_clean = df.dropna(subset=subset_cols_to_drop)
    else:
         df_clean = df.copy() # If state or industry missing, just make a copy
         print("Warning: Cannot drop rows based on missing 'state' or 'industry' as columns not found.")

else:
    print("Warning: Cannot check for missing key info as required columns are missing.")
    df_clean = df.copy() # If no key columns to check, just make a copy

"""## NLP"""

# Simple rule-based bucket mapping
RULE_BASED_BUCKETS = {
    "retail": ["retail","shop","store","wholesale"],
    "manufacturing": ["manufactur","factory","product","plant"],
    "construction": ["construction","building","contractor"],
    "food": ["poultry","food","bakery","meat","dairy"],
    "chemical": ["chemical","pharma","petro"],
    "textiles": ["textile","garment","clothing"],
    "services": ["service","it ","consult","hotel","hospital"],
    "transport": ["transport","logistic","shipping","rail","truck"]
}
def rule_bucket(s):
    t = str(s).lower()
    for b, kws in RULE_BASED_BUCKETS.items():
        for kw in kws:
            if kw in t:
                return b
    return "other"

if 'industry' in df.columns:
    df['bucket_rule'] = df['industry'].apply(rule_bucket)
    print(df['bucket_rule'].value_counts().head(20))

from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer
from sklearn.cluster import KMeans # Import KMeans
from sklearn.decomposition import TruncatedSVD # Import TruncatedSVD
import pandas as pd # Import pandas if not already available

def cluster_other_industries(df, n_clusters=6):
    mask = df['bucket_rule']=='other'
    corpus = df.loc[mask, 'industry'].fillna('unknown').astype(str)
    vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.8)
    X = vect.fit_transform(corpus)
    svd = TruncatedSVD(n_components=min(50, X.shape[1]-1))
    Xred = svd.fit_transform(X)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(Xred)
    df.loc[mask, 'bucket_nlp'] = ['cluster_'+str(x) for x in labels]
    return df

df = cluster_other_industries(df, n_clusters=6)
print(df['bucket_nlp'].value_counts().head())

# Install a compatible version of numpy before importing ydata-profiling
!pip install numpy<2.1 --force-reinstall

from ydata_profiling import ProfileReport # Import ProfileReport

# This produces a large HTML report you can open in notebook or browser
profile = ProfileReport(df, title="Resource Management EDA", minimal=True)
profile.to_file("eda_report.html")
print("EDA report saved: eda_report.html")

df.to_csv("cleaned_resource_management.csv", index=False)
print("Saved cleaned csv")

"""## -u kaleido"""

# Install kaleido for static image export
!pip install -U kaleido

RULE_BASED_BUCKETS = {
    "retail": ["retail", "shop", "store", "wholesale"],
    "manufacturing": ["manufactur", "factory", "product", "plant"],
    "construction": ["construction", "building", "contractor"],
    "food_processing": ["poultry", "food", "slaughter", "meat", "dairy", "bakery"],
    "chemical": ["chemical", "petro", "pharma"],
    "textiles": ["textile", "garment", "clothing"],
    "services": ["service", "it ", "consult"],
    "transport": ["transport", "logistic", "shipping", "rail", "truck"],
}

def rule_based_bucket(industry: str) -> str:
    t = str(industry).lower()
    for bucket, keywords in RULE_BASED_BUCKETS.items():
        for kw in keywords:
            if kw in t:
                return bucket
    return "other"

def cluster_industries(df: pd.DataFrame, n_clusters: int = 6, text_col: str = "industry"):
    corpus = df[text_col].astype(str).fillna("unknown").values
    vect = TfidfVectorizer(stop_words="english", ngram_range=(1,2), max_df=0.8)
    X = vect.fit_transform(corpus)
    svd = TruncatedSVD(n_components=min(100, X.shape[1]-1))
    Xred = svd.fit_transform(X)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df["nlp_cluster"] = kmeans.fit_predict(Xred)
    return df

def add_bucket_column(df: pd.DataFrame) -> pd.DataFrame:
    df["bucket_rule"] = df["industry"].apply(rule_based_bucket)
    df_other = df[df["bucket_rule"] == "other"].copy()
    if not df_other.empty:
        df_other = cluster_industries(df_other, n_clusters=6)
        df.loc[df_other.index, "bucket_nlp"] = "cluster_" + df_other["nlp_cluster"].astype(str)
    df["final_bucket"] = df["bucket_rule"].where(df["bucket_rule"]!="other", df.get("bucket_nlp","other"))
    return df

df.head

pd.read_csv("/content/cleaned_resource_management.csv")

import pandas as pd

# Assuming df is your merged dataset
print("Before filling missing values:")
print(df.isnull().sum().sort_values(ascending=False).head(10))

# Separate numeric and categorical columns
num_cols = df.select_dtypes(include=['number']).columns
cat_cols = df.select_dtypes(include=['object']).columns

# Fill numeric columns with median
for col in num_cols:
    df[col] = df[col].fillna(df[col].median())

# Fill categorical columns with mode or "Unknown"
for col in cat_cols:
    if not df[col].mode().empty:
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna("Unknown")

print("\nAfter filling missing values:")
print(df.isnull().sum().sort_values(ascending=False).head(10))

# Save cleaned dataset
output_path = "/content/cleaned_resource_management.csv"
df.to_csv(output_path, index=False)
print(f"\n‚úÖ Cleaned dataset saved at: {output_path}")

df.head

import pandas as pd

# üîπ 1. Check nulls before cleaning
print("Missing values before cleaning:")
print(df.isnull().sum().sort_values(ascending=False).head(10))

# üîπ 2. Drop rows with too many nulls (example: more than 50% null)
threshold = df.shape[1] // 2   # half the number of columns
df = df.dropna(thresh=threshold)

# üîπ 3. Drop columns if mostly null (example: >70% missing)
missing_fraction = df.isnull().mean()
cols_to_drop = missing_fraction[missing_fraction > 0.7].index
df = df.drop(columns=cols_to_drop)

print(f"\nDropped {len(cols_to_drop)} mostly-null columns.")

# üîπ 4. Fill remaining nulls
num_cols = df.select_dtypes(include=['number']).columns
cat_cols = df.select_dtypes(include=['object']).columns

# Numeric ‚Üí median
for col in num_cols:
    df[col] = df[col].fillna(df[col].median())

# Categorical ‚Üí mode (or "Unknown")
for col in cat_cols:
    if not df[col].mode().empty:
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna("Unknown")

# üîπ 5. Check again
print("\nMissing values after cleaning:")
print(df.isnull().sum().sort_values(ascending=False).head(10))

# üîπ 6. Save cleaned dataset
output_path = "/content/merged_dataset_cleaned.csv"
df.to_csv(output_path, index=False)
print(f"\n‚úÖ Cleaned dataset saved at: {output_path}")

# run this cell if packages are missing
!pip install -q matplotlib seaborn scikit-learn nltk gensim wordcloud pyLDAvis

pip install -U numpy scipy

!pip install "numpy>=2.0.0,<2.3.0"
!pip install "scipy>=1.14.0"

import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.cluster import KMeans
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset
df = pd.read_csv("/content/cleaned_resource_management.csv")

# Make sure relevant columns exist
if "nic_name" not in df.columns: # Changed from "industry" to "nic_name"
    raise ValueError("Column 'nic_name' not found!")
if "industry" not in df.columns:
    print("Warning: 'industry' column not found. Some downstream steps might be affected.")

# Clean text - Use 'nic_name' and keep more characters
def clean_text(s):
    s = str(s).lower()
    # Keep letters, numbers, spaces, and hyphens
    s = re.sub(r'[^a-z0-9\s-]', ' ', s)
    return s.strip()

# Use 'nic_name' for cleaning
df["text_for_nlp"] = df["nic_name"].apply(clean_text)

print("Head of cleaned text for NLP:\n", df["text_for_nlp"].head()) # Debug print

# Word frequency
# Adjusted max_features and added min_df
cv = CountVectorizer(stop_words="english", max_features=1000, min_df=5) # Increased max_features, adjusted min_df
try:
    X_cv = cv.fit_transform(df["text_for_nlp"]) # Use 'text_for_nlp'
    words = cv.get_feature_names_out()
    print(f"Vocabulary size (CountVectorizer): {len(words)}") # Debug print
    if len(words) > 0:
        counts = X_cv.toarray().sum(axis=0)
        freq = pd.DataFrame({"word": words, "count": counts}).sort_values("count", ascending=False)
        print("Top words:\n", freq.head(20)) # Print top 20 words
    else:
        print("Warning: CountVectorizer created an empty vocabulary.")
except ValueError as e:
    print(f"Error during CountVectorizer fit_transform: {e}")


# Wordcloud - Generate only if vocabulary is not empty
if 'words' in locals() and len(words) > 0:
    # Generate wordcloud from the cleaned text column
    wc = WordCloud(width=800, height=400, background_color="white").generate(" ".join(df["text_for_nlp"].dropna())) # Use 'text_for_nlp' and dropna
    plt.imshow(wc, interpolation="bilinear"); plt.axis("off"); plt.show()
else:
    print("Warning: Cannot generate wordcloud due to empty vocabulary.")


# TF-IDF + Clustering - Proceed only if TF-IDF can create a vocabulary
tfidf = TfidfVectorizer(stop_words="english", max_features=5000, min_df=5) # Adjusted max_features and min_df
try:
    X_tfidf = tfidf.fit_transform(df["text_for_nlp"]) # Use 'text_for_nlp'
    print(f"Vocabulary size (TfidfVectorizer): {len(tfidf.get_feature_names_out())}") # Debug print

    # Ensure there are enough samples and features for clustering
    if X_tfidf.shape[0] > 0 and X_tfidf.shape[1] > 0:
        kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
        df["industry_cluster"] = kmeans.fit_predict(X_tfidf)

        print("\nCluster counts:\n", df["industry_cluster"].value_counts())
    else:
        print("Warning: Insufficient data or features for TF-IDF clustering.")
        df["industry_cluster"] = -1 # Assign a default value

except ValueError as e:
    print(f"Error during TfidfVectorizer fit_transform or clustering: {e}")
    print("Cannot perform TF-IDF clustering due to vocabulary issue or other error.")
    df["industry_cluster"] = -1 # Assign a default value if clustering fails
except Exception as e: # Catch other potential errors during clustering
     print(f"An unexpected error occurred during TF-IDF clustering: {e}")
     df["industry_cluster"] = -1 # Assign a default value if clustering fails

df.to_csv("/content/resource_with_clusters.csv", index=False)
print("‚úÖ Saved clustered dataset: /content/resource_with_clusters.csv")

# Basic checks
print(df['industry'].head(10))
print("Nulls in industry:", df['industry'].isnull().sum())

# If missing or different name, inspect columns and choose a text column
# For safety, fill missing industry with empty string
df['industry'] = df['industry'].fillna("").astype(str)

def clean_text(s):
    s = str(s).lower()
    s = re.sub(r'[\r\n\t]', ' ', s)            # whitespace
    s = re.sub(r'[^a-z0-9\s]', ' ', s)         # keep alphanumeric + spaces
    s = re.sub(r'\s+', ' ', s).strip()
    return s

df['industry_clean'] = df['industry'].apply(clean_text)
df['industry_clean'].head()

"""## tokenization+stopwords+lemmatization"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
STOP = set(stopwords.words('english'))

def tokenize_and_filter(text):
    tokens = [t for t in text.split() if t not in STOP and len(t) > 1]
    return " ".join(tokens)

df['industry_tokens'] = df['industry_clean'].apply(tokenize_and_filter)
df['industry_tokens'].head()

"""## Exploratory text stats (word frequency & wordcloud)"""

print(df['industry'].head(10))
print(df['industry_clean'].head(10))

print(df['industry_clean'].head(20))
print("Total non-empty:", (df['industry_clean']!="").sum())

print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
print(df.head(5).T)   # transpose so we see all columns

if "industry" in df.columns:
    print("Non-empty rows in industry:", (df["industry"].astype(str).str.strip() != "").sum())
    print(df["industry"].dropna().unique()[:20])  # first 20 unique values
else:
    print("‚ö†Ô∏è Column 'industry' not found. Available:", df.columns.tolist())

def clean_text(s):
    s = str(s).lower()
    s = re.sub(r'[^a-z0-9\s-]', ' ', s)   # keep letters, numbers, hyphens
    s = re.sub(r'\s+', ' ', s).strip()
    return s

df['industry_clean'] = df['industry'].fillna("").apply(clean_text)
print(df['industry_clean'].head(10))

"""##TF-IDF representation (for clustering/topic modeling)"""

tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=5000)
X_tfidf = tfidf.fit_transform(df['industry_tokens'])
print("TF-IDF shape:", X_tfidf.shape)

# SVD (LSA)
svd = TruncatedSVD(n_components=100, random_state=42)
X_reduced = svd.fit_transform(X_tfidf)  # shape: (n_samples, 100)
print("Reduced shape:", X_reduced.shape)

# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import missingno as msno
import datetime as dt

# Read in the dataset
# Removed index_col='Unnamed: 0' as this column does not exist in the saved CSV
state = pd.read_csv('/content/merged_dataset.csv')
state.head()

state.dtypes

"""## data filtering"""

# IQR based filtering Technique:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/merged_dataset.csv')
df

# Use the correct column name
sns.boxplot(df["Division"])

import pandas as pd
import matplotlib.pyplot as plt

# Load your merged dataset
df = pd.read_csv("/content/merged_dataset.csv")

print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
print(df.head(3).T)   # see sample values for each column

import pandas as pd
import matplotlib.pyplot as plt
csv="/content/merged_dataset.csv"
df=pd.read_csv(csv) # Changed url to csv
print(df.head(5))
print(df.shape)

# Note: The following plotting code uses placeholder column names ('grade_test1', 'grade_test2', 'id')
# which are likely not present in your dataset. You will need to replace these
# with the actual column names from your DataFrame that you want to plot.

fig=plt.figure(figsize=(4,4), dpi=300)
test1 = fig.add_subplot(1,2,1)#nrows, ncols, index
test2 = fig.add_subplot(1,2,2)
#test3 = fig.add_subplot(2,2,2)
#pos is a three-digit integer where the first, second, and third integer are nrows,ncols, index.
# Replace 'grade_test1' and 'grade_test2' with actual column names from your df
# test1.boxplot(df["grade_test1"],showfliers=False)#method1 #showfliers=False(Hide outliers)
# test2.boxplot(df["grade_test2"],showfliers=True)
#test3.bar(df["grade_test2"],df["id"])


# Replace 'grade_test1' and 'grade_test2' with actual column names from your df
# plot = df[['grade_test1', 'grade_test2']].plot(kind='box', title='boxplot', showmeans=True)#method2


plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load your merged dataset
df = pd.read_csv("/content/merged_dataset.csv")

print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
print(df.head(3).T)   # see sample values for each column

# Get numeric columns only
numeric_cols = df.select_dtypes(include="number").columns.tolist()
print("Numeric columns:", numeric_cols)

df[numeric_cols].plot(
    kind="box",
    figsize=(12,6),
    title="Boxplots of numeric columns in Resource Management dataset",
    showmeans=True
)
plt.xticks(rotation=45)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Example: x = a categorical column, y = a numeric column
# Replace 'Division' and 'Main Workers - Total - Persons' with the actual column names you want to plot
sns.barplot(x="Division", y="Main Workers - Total -  Persons", data=df)

plt.xticks(rotation=45)
# Update title to reflect the columns being plotted
plt.title("Main Workers - Total - Persons by Division")
plt.show()

"""##NLP"""

CSV="/content/merged_dataset.csv"


import pandas as pd
df=pd.read_csv(csv)

df.head()

df["Marginal Workers - Rural - Males"].unique()

df.shape

df_text=df[["Marginal Workers - Rural - Males"]]
df_text.head()

"""## Load and clean ext data"""

import pandas as pd
import re

df = pd.read_csv("/content/merged_dataset.csv")

# choose the industry column (replace if different)
text_col = "industry" if "industry" in df.columns else df.columns[0]

def clean_text(s):
    s = "" if pd.isna(s) else str(s)
    s = s.lower()
    s = re.sub(r"[^a-z0-9\s-]", " ", s)  # keep letters/numbers/hyphen
    s = re.sub(r"\s+", " ", s).strip()
    return s

df["industry_clean"] = df[text_col].apply(clean_text)

"""##tokenization and stop words"""

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
STOP = set(ENGLISH_STOP_WORDS)

df["industry_tokens"] = df["industry_clean"].apply(
    lambda x: " ".join([w for w in x.split() if w not in STOP and len(w)>1])
)

"""## tfid vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_tfidf = tfidf.fit_transform(df["industry_tokens"])

"""## clustering industries"""

from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import MiniBatchKMeans

# Reduce dimensions for speed
# n_components must be <= n_features. X_tfidf.shape[1] is the number of features.
# Setting n_components to 20 as an example, adjust as needed but keep it <= X_tfidf.shape[1]
svd = TruncatedSVD(n_components=20, random_state=42)
X_reduced = svd.fit_transform(X_tfidf)  # shape: (n_samples, n_components)
print("Reduced shape:", X_reduced.shape)

# Cluster into k groups (choose based on elbow/silhouette)
k = 6
km = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1024, n_init=20)
df["industry_cluster"] = km.fit_predict(X_reduced)

print("\nCluster counts:")
print(df["industry_cluster"].value_counts())

"""## Interpret clusters (top terms)"""

import numpy as np

terms = tfidf.get_feature_names_out()
top_terms = {}
for ci in range(k):
    idxs = np.where(df["industry_cluster"]==ci)[0]
    mean_vec = X_tfidf[idxs].mean(axis=0).A1
    top_idxs = mean_vec.argsort()[::-1][:10]
    top_terms[ci] = [terms[i] for i in top_idxs]
    print(f"Cluster {ci} ‚Üí {top_terms[ci]}")

"""## Assign human-friendly labels"""

# Example mapping (adjust based on top_terms output)
cluster_to_label = {
    0: "Agriculture",
    1: "Retail",
    2: "Manufacturing",
    3: "Construction",
    4: "Services",
    5: "Other"
}
df["industry_group"] = df["industry_cluster"].map(cluster_to_label)

"""## save NLP labled dataset"""

df.to_csv("/content/resource_nlp_labeled.csv", index=False)
print("‚úÖ NLP-labeled dataset saved at /content/resource_nlp_labeled.csv")

import pandas as pd

# Load merged dataset (update path if needed)
df = pd.read_csv("/content/resource_nlp_labeled.csv")

print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
print(df.head(5).T)   # preview data

# Drop duplicate rows
df = df.drop_duplicates()

# Drop rows where industry column is missing
if "industry" in df.columns:
    df = df.dropna(subset=["industry"])
else:
    # If column name is different, replace "industry" with actual column
    df = df.dropna(subset=[df.columns[0]])

print("After removing duplicates & nulls:", df.shape)

import re

# Pick the right column (assuming it's named "industry")
text_col = "industry" if "industry" in df.columns else df.columns[0]

def clean_text(s):
    s = str(s).lower()                          # lowercase
    s = re.sub(r"[^a-z0-9\s-]", " ", s)         # keep letters, numbers, hyphen
    s = re.sub(r"\s+", " ", s).strip()          # collapse multiple spaces
    return s

df["industry_clean"] = df[text_col].apply(clean_text)
print(df["industry_clean"].head(10))

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
STOP = set(ENGLISH_STOP_WORDS)

def remove_stopwords(s):
    tokens = [t for t in s.split() if t not in STOP and len(t) > 1]
    return " ".join(tokens)

df["industry_tokens"] = df["industry_clean"].apply(remove_stopwords)
print(df["industry_tokens"].head(10))

# Drop rows that became empty after cleaning
df = df[df["industry_tokens"].str.strip() != ""].reset_index(drop=True)

print("Final cleaned dataset shape:", df.shape)

df.to_csv("/content/cleaned_resource_management.csv", index=False)
print("‚úÖ Cleaned dataset saved as /content/cleaned_resource_management.csv")

